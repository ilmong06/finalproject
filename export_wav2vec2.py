import os
os.environ["SPEECHBRAIN_LOCAL_FILE_STRATEGY"] = "copy"

from flask import Flask, request, jsonify
from flask_cors import CORS
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
from speechbrain.pretrained import SpeakerRecognition
from pydub import AudioSegment
import torchaudio
import torch
import uuid

app = Flask(__name__)
CORS(app)

# âœ… ì˜ì–´ STT ëª¨ë¸
processor_en = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model_en = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

# âœ… í•œêµ­ì–´ STT ëª¨ë¸
processor_ko = Wav2Vec2Processor.from_pretrained("kresnik/wav2vec2-large-xlsr-korean")
model_ko = Wav2Vec2ForCTC.from_pretrained("kresnik/wav2vec2-large-xlsr-korean")

# âœ… í™”ì ì„ë² ë”© ëª¨ë¸ (x-vector)
speaker_model = SpeakerRecognition.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb",
    savedir=None  # â†’ ìë™ ìºì‹œ ê²½ë¡œ ì‚¬ìš©
)

@app.route("/stt", methods=["POST"])
def transcribe():
    if "file" not in request.files:
        return jsonify({"error": "No file part"}), 400

    file = request.files["file"]
    if file.filename == "":
        return jsonify({"error": "No selected file"}), 400

    lang = request.form.get("lang", "ko")  # ê¸°ë³¸ê°’: í•œêµ­ì–´

    temp_filename = f"temp_{uuid.uuid4().hex}.wav"
    file.save(temp_filename)

    try:
        # âœ… WAV ì¬ì¸ì½”ë”© (torchaudio ì˜¤ë¥˜ ë°©ì§€)
        audio = AudioSegment.from_file(temp_filename)
        audio = audio.set_frame_rate(16000).set_channels(1)
        audio.export(temp_filename, format="wav")

        # âœ… torchaudio ë¡œë”©
        waveform, sample_rate = torchaudio.load(temp_filename)

        # âœ… Wav2Vec2 STT
        transcription = ""
        if lang == "ko":
            print("ğŸŒ í•œêµ­ì–´ ì¸ì‹ ì¤‘...")
            input_values = processor_ko(waveform.squeeze(), sampling_rate=16000, return_tensors="pt").input_values
            with torch.no_grad():
                logits = model_ko(input_values).logits
            predicted_ids = torch.argmax(logits, dim=-1)
            transcription = processor_ko.batch_decode(predicted_ids)[0]
        else:
            print("ğŸŒ ì˜ì–´ ì¸ì‹ ì¤‘...")
            input_values = processor_en(waveform.squeeze(), sampling_rate=16000, return_tensors="pt").input_values
            with torch.no_grad():
                logits = model_en(input_values).logits
            predicted_ids = torch.argmax(logits, dim=-1)
            transcription = processor_en.batch_decode(predicted_ids)[0]

        print("ğŸ“£ ì¸ì‹ëœ ë¬¸ì¥:", transcription)

        # âœ… í™”ì íŠ¹ì§• ì¶”ì¶œ (x-vector)
        print("ğŸ§¬ í™”ì íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        speaker_embedding = speaker_model.encode_batch(waveform)
        speaker_vector = speaker_embedding.squeeze().tolist()

    except Exception as e:
        print("âŒ ì˜¤ë¥˜ ë°œìƒ:", e)
        return jsonify({"error": str(e)}), 500
    finally:
        os.remove(temp_filename)

    return jsonify({
        "text": transcription,
        "speaker_vector": speaker_vector
    })

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
