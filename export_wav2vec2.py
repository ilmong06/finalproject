import os
os.environ["SPEECHBRAIN_LOCAL_FILE_STRATEGY"] = "copy"

from flask import Flask, request, jsonify
from flask_cors import CORS
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
from speechbrain.pretrained import SpeakerRecognition
from pydub import AudioSegment
from pathlib import Path
import torchaudio
from torchvision.models import resnet18
import torch.nn as nn
import torch.nn.functional as F
from sklearn.metrics.pairwise import cosine_similarity
import torch
import uuid
import json
import numpy as np
import matplotlib.pyplot as plt

app = Flask(__name__)
CORS(app)

# âœ… STT ëª¨ë¸
processor_en = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model_en = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")
processor_ko = Wav2Vec2Processor.from_pretrained("kresnik/wav2vec2-large-xlsr-korean")
model_ko = Wav2Vec2ForCTC.from_pretrained("kresnik/wav2vec2-large-xlsr-korean")

# âœ… í™”ì ì„ë² ë”© ëª¨ë¸
speaker_model = SpeakerRecognition.from_hparams(source="speechbrain/spkrec-ecapa-voxceleb")

# âœ… ì €ì¥ íŒŒì¼
TEMP_VECTORS_FILE = "registered_vectors.json"
FINAL_VECTOR_FILE = "registered_speaker.json"
KEYWORD_VECTOR_FILE = "registered_keyword_vectors.json"

# âœ… í™”ì ë“±ë¡
@app.route("/register", methods=["POST"])
def register_speaker():
    if "file" not in request.files:
        return jsonify({"error": "No file part"}), 400
    file = request.files["file"]
    if file.filename == "":
        return jsonify({"error": "No selected file"}), 400

    temp_filename = f"register_{uuid.uuid4().hex}.wav"
    file.save(temp_filename)

    try:
        audio = AudioSegment.from_file(temp_filename).set_frame_rate(16000).set_channels(1)
        audio.export(temp_filename, format="wav")
        waveform, _ = torchaudio.load(temp_filename)
        embedding = speaker_model.encode_batch(waveform).squeeze().numpy()
        norm_vector = embedding / np.linalg.norm(embedding)

        vectors = []
        if os.path.exists(TEMP_VECTORS_FILE):
            with open(TEMP_VECTORS_FILE, "r") as f:
                vectors = json.load(f)
        vectors.append(norm_vector.tolist())
        with open(TEMP_VECTORS_FILE, "w") as f:
            json.dump(vectors, f)

        print(f"ğŸ§¾ í˜„ì¬ ë“±ë¡ëœ í™”ì ë²¡í„° ìˆ˜: {len(vectors)}")

        if len(vectors) == 4:
            vectors_np = np.array(vectors)
            mean_vector = np.mean(vectors_np, axis=0)
            final_vector = mean_vector / np.linalg.norm(mean_vector)
            with open(FINAL_VECTOR_FILE, "w") as f:
                json.dump(final_vector.tolist(), f)
            os.remove(TEMP_VECTORS_FILE)
            print("âœ… í™”ì ì²´ì¸ í™•ì • ì™„ë£Œ")
            print("âœ… í‰ê·  ë²¡í„°:", final_vector.tolist())

            # ì‹œê°í™”
            plt.figure(figsize=(14, 6))
            for i, vec in enumerate(vectors_np):
                plt.plot(vec, label=f"Registered Vector {i+1}", linestyle='--', alpha=0.6)
            plt.plot(final_vector, label="Mean Speaker Vector", linewidth=2.5)
            plt.title("ğŸ“Š í™”ì ë²¡í„° í‰ê·  ì²˜ë¦¬ ê²°ê³¼")
            plt.xlabel("ì°¨ì› ì¸ë±ìŠ¤")
            plt.ylabel("ê°’")
            plt.legend()
            plt.grid(True)
            plt.tight_layout()

            return jsonify({"message": "í™”ì ë“±ë¡ ì™„ë£Œ (4/4)"})
        else:
            return jsonify({"message": f"ë“±ë¡ {len(vectors)}/4 ì™„ë£Œ"})

    except Exception as e:
        return jsonify({"error": str(e)}), 500
    finally:
        os.remove(temp_filename)

# âœ… í‚¤ì›Œë“œ ë“±ë¡
@app.route("/register_keyword", methods=["POST"])
def register_keyword():
    keyword = request.form.get("keyword")
    if not keyword or "file" not in request.files:
        return jsonify({"error": "í‚¤ì›Œë“œ ë˜ëŠ” íŒŒì¼ ì—†ìŒ"}), 400

    save_dir = Path("data/custom") / keyword
    save_dir.mkdir(parents=True, exist_ok=True)

    existing = list(save_dir.glob("*.wav"))
    index = len(existing) + 1

    raw_path = save_dir / f"raw_{index}.wav"
    fixed_path = save_dir / f"record_{index}.wav"

    try:
        file = request.files["file"]
        file.save(str(raw_path))

        audio = AudioSegment.from_file(str(raw_path))
        audio = audio.set_frame_rate(16000).set_channels(1)
        audio.export(str(fixed_path), format="wav")
        os.remove(str(raw_path))

        if index == 6:
            os.system("python train_fewshot.py")
            return jsonify({"message": f"{keyword} í‚¤ì›Œë“œ 6ê°œ ë…¹ìŒ ì™„ë£Œ â†’ ëª¨ë¸ í•™ìŠµ ì‹œì‘ë¨"}), 200
        else:
            return jsonify({"message": f"{keyword} í‚¤ì›Œë“œ {index}/6 ë…¹ìŒ ì €ì¥ ì™„ë£Œ"}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

# âœ… STT + í™”ì + í‚¤ì›Œë“œ ì¸ì¦
@app.route("/stt", methods=["POST"])
def transcribe():
    if "file" not in request.files:
        return jsonify({"error": "No file part"}), 400

    file = request.files["file"]
    if file.filename == "":
        return jsonify({"error": "No selected file"}), 400

    lang = request.form.get("lang", "ko")
    temp_filename = f"temp_{uuid.uuid4().hex}.wav"
    file.save(temp_filename)

    try:
        audio = AudioSegment.from_file(temp_filename).set_frame_rate(16000).set_channels(1)
        audio.export(temp_filename, format="wav")
        waveform, _ = torchaudio.load(temp_filename)

        print("í™”ì íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        speaker_embedding = speaker_model.encode_batch(waveform)
        speaker_vector = speaker_embedding.squeeze().numpy()
        norm_vector = speaker_vector / np.linalg.norm(speaker_vector)

        if not os.path.exists(FINAL_VECTOR_FILE):
            return jsonify({"error": "ë“±ë¡ëœ í™”ìê°€ ì—†ìŠµë‹ˆë‹¤"}), 403
        with open(FINAL_VECTOR_FILE, "r") as f:
            registered_vector = np.array(json.load(f))
        speaker_similarity = np.dot(norm_vector, registered_vector)
        print(f"í™”ì ìœ ì‚¬ë„: {speaker_similarity:.4f}")
        if speaker_similarity < 0.7:
            return jsonify({"error": "í™”ì ì¸ì¦ ì‹¤íŒ¨"}), 403

        from sklearn.metrics.pairwise import cosine_similarity
        import torch.nn as nn
        import torch.nn.functional as F

        class ResKeywordNet(nn.Module):
            def __init__(self):
                super().__init__()
                base_model = resnet18(pretrained=False)
                base_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
                self.features = nn.Sequential(*list(base_model.children())[:-1])
                self.fc = nn.Linear(512, 128)

            def forward(self, x):
                x = self.features(x)
                x = x.view(x.size(0), -1)
                return self.fc(x)

        def extract_mel(filepath, target_length=232):
            waveform, sr = torchaudio.load(filepath)
            waveform = torchaudio.functional.resample(waveform, sr, 16000)
            mel = torchaudio.transforms.MelSpectrogram(
                sample_rate=16000,
                n_fft=1024,
                hop_length=160,
                n_mels=80
            )(waveform)
            if mel.shape[-1] < target_length:
                pad = target_length - mel.shape[-1]
                mel = F.pad(mel, (0, pad))
            elif mel.shape[-1] > target_length:
                mel = mel[:, :, :target_length]
            return mel.unsqueeze(0)

        # âœ… í‚¤ì›Œë“œ ìœ ì‚¬ë„ íŒë‹¨
        if not os.path.exists("fewshot_model.pt") or not os.path.exists("label_map.json"):
            return jsonify({"error": "Few-shot ëª¨ë¸ ë˜ëŠ” ë¼ë²¨ë§µ ì—†ìŒ"}), 403

        mel = extract_mel(temp_filename)
        model = ResKeywordNet()
        checkpoint = torch.load("fewshot_model.pt", map_location="cpu")
        model.load_state_dict(checkpoint["model"])
        model.eval()

        with open("label_map.json", "r", encoding="utf-8") as f:
            label_map = json.load(f)

        with torch.no_grad():
            emb = model(mel).numpy()

        triggered_keyword = None
        best_score = 0
        print("\nğŸ“Š í‚¤ì›Œë“œ ìœ ì‚¬ë„ (Cosine Similarity):")
        for keyword, vec in label_map.items():
            score = cosine_similarity(emb, [vec])[0][0]
            print(f"ğŸ”¸ '{keyword}' â†” ì…ë ¥ ìŒì„± : {score:.4f}")
            if score > best_score:
                best_score = score
                triggered_keyword = keyword

        if best_score < 0.7:
            return jsonify({
                "error": "í‚¤ì›Œë“œ ì¸ì¦ ì‹¤íŒ¨ (Few-shot)",
                "triggered_keyword": triggered_keyword,
                "similarity": round(best_score, 4)
            }), 403



        # âœ… STT ìˆ˜í–‰
        transcription = ""
        if lang == "ko":
            print("ğŸŒ í•œêµ­ì–´ ì¸ì‹ ì¤‘...")
            input_values = processor_ko(waveform.squeeze(), sampling_rate=16000, return_tensors="pt").input_values
            with torch.no_grad():
                logits = model_ko(input_values).logits
            predicted_ids = torch.argmax(logits, dim=-1)
            transcription = processor_ko.batch_decode(predicted_ids)[0]
        else:
            print("ğŸŒ ì˜ì–´ ì¸ì‹ ì¤‘...")
            input_values = processor_en(waveform.squeeze(), sampling_rate=16000, return_tensors="pt").input_values
            with torch.no_grad():
                logits = model_en(input_values).logits
            predicted_ids = torch.argmax(logits, dim=-1)
            transcription = processor_en.batch_decode(predicted_ids)[0]

        print(f"ğŸ“£ ì¸ì‹ëœ ë¬¸ì¥: {transcription}")
        return jsonify({
            "text": transcription,
            "triggered_keyword": triggered_keyword,
            "similarity": round(best_score, 4),
            "speaker_vector": norm_vector.tolist()
        })


    except Exception as e:
        return jsonify({"error": str(e)}), 500
    finally:
        os.remove(temp_filename)

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
