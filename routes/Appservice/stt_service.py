import os
import json
import numpy as np
from scipy.spatial.distance import cosine
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import torch
import torchaudio

# ğŸ”µ ëª¨ë¸ ë¡œë”©
processor_ko = Wav2Vec2Processor.from_pretrained("kresnik/wav2vec2-large-xlsr-korean")
model_ko = Wav2Vec2ForCTC.from_pretrained("kresnik/wav2vec2-large-xlsr-korean")

SPEAKER_VECTOR_FILE = 'model/registered_speaker.json'
KEYWORD_VECTOR_FILE = 'model/registered_keyword_vectors.json'

# ğŸ”µ STT ë° ì¸ì¦ ì²˜ë¦¬
def process_stt(file, lang):
    """
    ìŒì„±íŒŒì¼ì„ ë°›ì•„ì„œ í™”ì ì¸ì¦ + í‚¤ì›Œë“œ ì¸ì¦ + STT ê²°ê³¼ ë°˜í™˜
    """
    # 1. í™”ì ì¸ì¦
    speaker_passed = check_speaker(file)
    if not speaker_passed:
        return {"error": "í™”ì ì¸ì¦ ì‹¤íŒ¨"}

    # 2. í‚¤ì›Œë“œ ì¸ì¦
    keyword_passed = check_keyword(file)
    if not keyword_passed:
        return {"error": "í‚¤ì›Œë“œ ì¸ì¦ ì‹¤íŒ¨"}

    # 3. STT ë³€í™˜
    stt_text = stt(file, lang)
    return {"result": stt_text}

# ğŸ”µ í™”ì ì¸ì¦
def check_speaker(file):
    if not os.path.exists(SPEAKER_VECTOR_FILE):
        return False

    with open(SPEAKER_VECTOR_FILE, 'r') as f:
        registered = json.load(f)

    # ì‹¤ì œë¡œëŠ” ìŒì„±ì—ì„œ speaker vector ì¶”ì¶œí•´ì•¼ í•¨
    dummy_vector = np.random.rand(512)

    for uuid, registered_vector in registered.items():
        dist = cosine(dummy_vector, np.array(registered_vector))
        if dist < 0.5:  # threshold
            return True

    return False

# ğŸ”µ í‚¤ì›Œë“œ ì¸ì¦
def check_keyword(file):
    if not os.path.exists(KEYWORD_VECTOR_FILE):
        return False

    with open(KEYWORD_VECTOR_FILE, 'r') as f:
        registered = json.load(f)

    # ì‹¤ì œë¡œëŠ” ìŒì„±ì—ì„œ keyword vector ì¶”ì¶œí•´ì•¼ í•¨
    dummy_vector = np.random.rand(512)

    for keyword, registered_vector in registered.items():
        dist = cosine(dummy_vector, np.array(registered_vector))
        if dist < 0.5:  # threshold
            return True

    return False

# ğŸ”µ STT
def stt(file, lang):
    waveform, sample_rate = torchaudio.load(file)
    input_values = processor_ko(waveform.squeeze(), return_tensors="pt", sampling_rate=sample_rate).input_values
    logits = model_ko(input_values).logits
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor_ko.batch_decode(predicted_ids)[0]
    return transcription
